#机器学习常见知识汇总与详解。
***
田晓彬
xiaobin9652@163.com
***
#线性回归
&emsp;&emsp;线性回归试图学习得到一个线性模型以尽可能准确地预测输入值的输出标记。为了简化描述，我们讨论输入值只有一个属性的二分类情况，$D = \{(x_i,y_i)|i \in 1,2,...,n\}$。
&emsp;&emsp;我们很容易可以得到 $y$ 关于 $x$ 的线性模型 $y_i = wx_i + b$。但在实际应用中我们不一定能找到完全合适的参数 $w、b$ 来使等式完全成立，所以我们将线性回归的学习目标定义为 $$ f(x_i) = wx_i + b \qquad \text{s.t.} f(x_i) \approx y_i. $$
&emsp;&emsp;接下来需要确定参数 $w、b$ ，在此引入均方误差来度量 $f(x_i)$ 和 $y_i$ 之间的差距，并最小化均方误差来确定最优的参数 $$ \argmin_{w,b} {\sum_{i=1}^{n}{(f(x_i) - y_i)^2}} = \argmin_{w,b} {\sum_{i=1}^{n}{(y_i - wx_i - b)^2}}.$$
&emsp;&emsp;有均方误差的公式可以看出，均方误差具有很好的集合意义，即对应了几何上常用的欧几里得距离。在线性回归中引入均方误差，即试图找到最优参数对应的一条直线，使得所有样本点到该直线的欧式距离之和最小。线性回归的优化函数可以通过最小二乘法进行求解，具体的求解步骤在此不再叙述。
***
#逻辑回归
####1.逻辑回归不是回归问题
&emsp;&emsp;输入变量和输出变量均为连续变量的预测问题称之为回归问题；输出变量为有限个离散变量的预测问题称之为分类问题。
&emsp;&emsp;虽然逻辑回归叫做回归，但是它并不是一个回归问题。在逻辑回归中，因变量取值是一个二元分布，模型学习得到的是$E[y|x;θ]$,即给定自变量和超参数以后，得到因变量的期望。
####2.逻辑回归的详细解释
&emsp;&emsp;在线性回归中，我们通过求解$z=θx+μ$来近似$z$,其中$μ$是误差项。若是希望用回归来处理分类任务，我们需要将线性回归系统的输出$z$转换成0/1值。首先想到的就是单位跃阶函数：$$
y = 
\begin{cases}
0,   & \text{$z$<0;}\\
0.5, & \text{$z$=0;}\\
1,   & \text{$z$>0,}  
\end{cases}
$$其中0叫做阈值（可以根据具体情况换成其它值），若预测值大于阈值则为正例，若预测值小于阈值则为负例，若预测值等于阈值则可以任意判别。
&emsp;&emsp;但是很显然，单位跃阶函数是不可微的，我们希望找到一个与单位跃阶函数相似又可微的函数。$Sigmod$函数定义如下：$$ y= \frac{1}{1+{e}^{-z}} $$ 其中$z=w^Tx+b$。由于$sigmod$函数的特性，它输出的结果不再是预测结果，而是预测结果为正例的概率，预测结果为负例的概率为：$$ 1-y = \frac{{e}^{-z}}{1+{e}^{-z}}$$ 两者的比值 $$ \frac{y}{1-y} = \frac{1}{{e}^{-z}} $$ 就是 $x$ 作为正例的可能性,成为“几率”。对几率取对数则得到对数几率：$$ \ln\frac{y}{1-y} = z = w^Tx+b $$ 若将$f(z)$视为后验概率$p(y=1|{x})$,$$ \ln \frac{p(y=1 \mid x)}{p(y=0\mid x)} = w^Tx+b$$ 显然有 $$ p(y=1\mid x) = \frac{e^{w^Tx}}{1+e^{w^Tx}} $$ $$ p(y=0\mid x) = \frac{1}{1+e^{w^Tx}} $$
&emsp;&emsp;由概率论可知，$$ p(y_i\mid x_i;w,b) = {(p(y=1\mid x;w,b))}^{y_i} \times {(1-p(y=0\mid x;w,b))}^{1-y_i} $$ 
&emsp;&emsp;根据最大似然估计可知，对于很多个样本 $ x_i $ ,若希望求得最合适的参数 $ w,b $ 使正确预测的概率最大，则求得合适的参数使得所有样本正确预测概率的乘积最大，即： $$ {l(w,b)} = \prod_{i=1}^{N}{p(y_i \mid x_i;w,b)} $$ 等式两边同时取对数似然 $$ {L(w,b)} = \ln{l(w,b)} = \sum_{i=1}^{N}{(y_i\ln{p_1(x_i;w,b)}+(1-y_i) \ln{p_0(x_i;w,b)})} $$ 化简可得 $$ L(w,b) = \sum_{i=1}^{N}{(y_i(w^Tx+b)-\ln{(1+e^{w^Tx+b})})} $$ 由于我们习惯于最小化一个函数，所以 $ L(w,b) $ 划为 $$ \min_{w,b}{L(w,b)} = \sum_{i=1}^{N}{(-y_i(w^Tx+b)+\ln{(1+e^{w^Tx+b})})} $$ 这个函数就是逻辑回归的损失函数，被叫做交叉熵损失函数。
####3.逻辑回归损失函数的求解
&emsp;&emsp;求解该损失函数需要使用梯度下降方法，在此不做详细介绍。
####4.逻辑回归适用性
1. **用于概率预测**：逻辑回归的输出为正例的可能性，即概率；
2. **用于分类问题**：逻辑回归的输出虽然为概率，但是我们可以通过添加阈值来对逻辑回归的输出进行分类。不同的分类问题可能对应不同的阈值，需要根据具体情况来进行具体确定；
3. **仅能用于线性问题**：只有当目标和特征是线性关系时，才能使用逻辑回归；
4. **特征不需要满足独立条件假设**：各个特征之间不需要满足条件独立假设，但是各个特征的贡献需要独立计算。
####5.逻辑回归和线性回归的异同。
&emsp;&emsp;逻辑回归是分类问题，线性回归是回归问题，这是两者最本质的区别；逻辑回归中的因变量是离散的，而线性回归中的因变量的连续的；在自变量 $x$ 和超参数 $w,b$ 确定的情况下，逻辑回归可以看做广义线性模型在因变量 $y$ 服从二元分布的一个特殊情况，线性回归的求解使用最小二乘法，可以认为是因变量 $y$ 服从正态分布。
&emsp;&emsp;两者也有相同的地方，首先二者都是在使用极大似然估计来对训练样本进行建模。线性回归使用最小二乘法进行求解，实际上就是在自变量 $x$ 和超参数 $w,b$ 确定、因变量 $y$ 服从正态分布的情况下，使用极大似然估计的一个化简；而逻辑回归中通过对似然函数 $$ {l(w,b)} = \prod_{i=1}^{N}{p(y_i\mid x_i;w,b)} = \prod_{i=1}^{N}{(p(x_i)^{y_i}(1-p(x_i))^{(1-y_i)})} $$ 的学习，得到最优超参数 $w,b$ 。另外，二者在求解超参数的过程中，都可以使用梯度下降算法。
****
#决策树
####1.决策树的基本思想
&emsp;&emsp;决策树有若干个内部节点和叶子节点。叶子节点对应决策结果，内部节点对应一个特征或者属性。从决策树的根节点开始，样本被划分到不同的子节点中，再根据子节点的特征进行进一步划分，直至所有样本都被归到某一类别（叶子节点）中。决策树学习的目的是为了产生一颗泛化能力强，即处理未见数据能力强的决策树。决策树的生成包含特征选择，树的构造，树的剪枝三个过程。下面详细介绍。
####2.决策树的三种构造方法
&emsp;&emsp;再具体情况中，我们即希望决策树能有效地拟合数据，达到良好的分类效果，同时又希望可以控制决策树的复杂度，使模型具有一定的泛化能力。而决策树的选择的可能性有很多种，从中选取最优的决策树是一个NP难问题。我们通常使用启发式学习的方法来构造一颗满足启发式条件的决策树。根据选取特征的不同量化评估指标，可以将常用的决策树构造算法分为三种：ID3、C4.5、CART。
1. **ID3-最大信息增益**
&emsp;&emsp;信息熵是度量样本集合纯度最常用的一种指标。样本集合 $D$ 的信息熵定义为 $$ Ent(D)=-\sum_{k=1}^{K}{p_k\log_2p_k} $$ 其中样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k$。 $Ent(D)$ 的值越小，则 $D$ 的纯度越高。
&emsp;&emsp;假设离散属性 $a$ 有$V$个可能的取值，$D^v$ 为 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本集。不同的 $D^v$ 包含的样本数不同，给子集赋予权重 $D^v / D$，即样本数越多的子集影响越大，于是可以计算属性 $a$ 对样本集 $D$ 进行划分所获得的的信息增益 $$ Gain(D,a)=Ent(D)-\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Ent(D^v)} $$ 信息增益越大，则意味着使用属性 $a$ 来进行划分所获得的的信息纯度提升越大。我们使用信息增益来确定决策树的属性划分选择，即找到所有为确定属性中，信息增益最大的属性进行划分决策树。
2. **C4.5-最大信息增益比**
&emsp;&emsp;C4.5算法不直接使用信息增益，而是使用增益率来选择最优划分属性。增益率定义为： $$ Gainratio(D,a)=\frac{Gain(D,a)}{H_a(D)} $$ 其中 $$ H_a(D)=-\sum_{v=1}^{V}{\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}} $$ 称为属性的固有值，属性 $a$ 的可能取值的数目越多，则 $H_a(D)$ 的值越大。
&emsp;&emsp;增益率准则对可取值数目较少的属性有所偏好，C4.5算法并不直接使用增益率最大的候选划分属性，而先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。
3. **CART-最小基尼指数**
&emsp;&emsp;CART决策树使用基尼指数来选择划分属性。数据集 $D$ 的纯度可以用基尼值来度量： $$ Gini(D) = \sum_{k=1}^{K}{\sum_{{k}^{'}\neq{k}}{p_kp_{k^{'}}}} = 1-\sum_{k=1}^{K}{p_{k}^{2}} $$ $Gini(D)$ 反映了从数据集 $D$ 中随机抽取两个样本，其标记不一致的概率。 因此 $Gini(D)$ 越小，则数据集 $D$ 的纯度越高。由此可知属性 $a$ 的基尼指数定义为： $$ Giniindex (D,a)  =\sum_{v=1}^{V}{\frac{|D^v|}{|D|}Gini(D^v)} $$
&emsp;&emsp;在划分候选属性集合的时候，选择使划分后基尼指数最小的属性作为最优划分属性。
注：三种特征选择方法中，只有基尼指数为选择最小的，信息增益和信息增益比都为选择最大的。
####3.三种决策树构造准则的异同
&emsp;&emsp;首先，ID3采用信息增益作为评价指标，但是信息增益反映的是给定条件后不确定性减少的程度，这个值会倾向于特征取值多的特征。特征取值越多意味着确定性越高，也就是条件熵越小，即信息增益越大。但通常特征数过多的特征的泛化能力是非常弱的。C4.5通过引入信息增益比，在一定程度上对取值比较多的特征进行惩罚，避免出现ID3中的过拟合的特征，提高决策树的泛化能力。
&emsp;&emsp;其次，从样本的角度，ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。
&emsp;&emsp;从应用角度，ID3和C4.5只能用于分类任务，CART(Classification and Regression Tree)还可以用于回归任务（使用最小平方误差准侧）。
&emsp;&emsp;此外，ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理。ID3和C4.5可以在每个节点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个节点只会产生两个分支，而且每个特征可以被重复使用。ID3和C4.5通过剪枝来权衡决策树的准确性和泛化能力，而CART直接利用全部的数据发现所有可能的树结构进行对比。
####4.决策树的剪枝策略
&emsp;&emsp;一颗完全生长的决策树会面临过拟合这个严重的问题。这是因为在决策树的学习过程中，为了尽可能正确的确定划分训练样本，把训练样本的一些特点当作所有数据具有的一般特性。剪枝是决策树学习算法中应对过拟合的主要手段。决策树的剪枝通常有两种手段，预剪枝和后剪枝。
1. **预剪枝**
&emsp;&emsp;预剪枝是在对决策树中节点进行扩展之前进行评估，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。标记的类别为当前节点集合中数量最多的类别。预剪枝对何时停止决策树的生长有以下几种方法：
&emsp;&emsp;1）当树达到一定深度的时候，停止树的生长；
&emsp;&emsp;2）当到达当前节点的样本数量小于某个阈值的时候，停止树的生长；
&emsp;&emsp;3）计算每次分裂对测试集的准确度会有提升，当这个提升小于某个阈值的时候，不在继续扩展。
&emsp;&emsp;预剪枝使得决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还显著减少了决策树的时间开销。但决策树的预剪枝有一定的局限性，可能有欠拟合的风险，虽然当前的划分会导致测试集的准确率降低，但是后续的划分可能会使准确率有显著的上升。
2. **后剪枝**
&emsp;&emsp;后剪枝先从训练集中生成一颗完整的决策树，然后自底向上地对非叶子节点进行考察。首先将该节点对应的子树替换成叶子节点，叶子节点的类别为当前节点集合中数量较多的类别。然后计算替换后的决策树在测试集上的泛化能力，若泛化能力提升，那么将该节点剪枝。相对于预剪枝，后剪枝方法通常可以得到泛化能力强的决策树，但时间开销会更大。
&emsp;&emsp;常见的后剪枝方法包括：错误率降低剪枝（REP）、悲观剪枝（PEP）、代价复杂度剪枝（CCP）、最小误差剪枝（MEP）、CVP、OPP等方法。
####5.决策树的连续值处理
&emsp;&emsp;C4.5和CART都可以对连续值进行处理。这里介绍C4.5使用的简单二分法对属性进行划分。对于样本集 $D$ 和连续属性 $a$ ，假定 $a$ 在 $D$ 上出现了 $n$ 个不同的取值，首先将这 $n$ 个不同的取值进行排序。假定划分点为 $t$ ，基于划分点 $t$ 我们可以将数据集划分成两部分 $D_t^-$和$D_t^+$ ，其中 $D_t^-$ 是在属性 $a$ 上取值不大于 $t$ 的样本，$D_t^+$ 是在属性 $a$ 上取值大于 $t$ 的样本。因此对于属性 $a$ ，我们可以得到 $n-1$ 个候选划分点集合 $$ T_a = \left\{ \frac{a^i+a^{i+1}}{2} \mid 1 \leq i \leq n-1 \right\} $$ 即把区间 $ \left[ a^i,a^{i+1} \right) $ 的中位点作为候选划分点。那么属性 $a$ 在数据集 $D$ 上的信息增益为 $$ Gain(D,a) = \max_{t\in T_a}{Gain(D,a,t) = \max_{t\in T_a}{Ent(D) - \sum_{\lambda \in \left\{ -,+\right\}}{\frac{|D_t^\lambda|}{|D|} Ent(D_t^\lambda)}}} $$ 
&emsp;&emsp;基于上述公式，我们可以得到在所有划分点集合中，使信息增益最大的划分点。根据此划分点便可以将属性 $a$ 分为两个离散值。
&emsp;&emsp;与离散属性不同，若当前节点划分属性为连续属性，该属性还可以可以作为其后代节点的划分属性，即划分点不同。
####6.决策树的缺失值处理
&emsp;&emsp;当数据集中有缺失值时，需要解决两个问题：如何在属性值确实的情况下进行划分属性选择？给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分。C4.5和CART都可以对缺失值进行处理。这里介绍C4.5使用的处理方法。
&emsp;&emsp;给定样本集 $D$ 和连续属性 $a$，令 $\widetilde{D}$ 表示属性 $a$ 在样本集 $D$ 上没有确实的样本子集。令 $\widetilde{D}^v$ 表示 $\widetilde{D}$ 在属性 $a$ 上取值为 $a^v$ 的样本子集， $\widetilde{D}_k$ 表示 $\widetilde{D}$ 中属于第 $k$ 类的样本子集。假定给每一个样本都赋予一个权重 $w_x$ ,在决策树学习的开始阶段，根节点各个样本的权重都初始化为1。
&emsp;&emsp;定义 $$ \rho = \frac{\sum_{x \in \widetilde{D}}{w_x}}{\sum_{x \in D}{w_x}} $$ $$ \widetilde{p}_k =\frac{\sum_{x \in \widetilde{D}_k}{w_x}}{\sum_{x \in \widetilde{D}}{w_x}} $$ $$ \widetilde{r}_v =\frac{\sum_{x \in \widetilde{D}^v}{w_x}}{\sum_{x \in \widetilde{D}}{w_x}} $$ 有公式可以看出，对于属性 $a$ ，$\rho$ 表示无缺失值样本所占的比例， $\widetilde{p}_k$ 表示无缺失值样本中第 $k$ 类样本所占的比例， $\widetilde{r}_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占的比例。
&emsp;&emsp;基于上述定义，信息增益的计算公式为： $$ Gain(D,a) = \rho \times Gain(\widetilde{D},a) = \rho \times \left( Ent(\widetilde{D}) -\sum_{v=1}^{V}{Ent(\widetilde{D}^v)} \right) $$ 其中 $$ Ent(\widetilde{D}) = -\sum_{k=1}^{K}{\widetilde{p}_k \log_2 {\widetilde{p}_k}} $$
&emsp;&emsp;对于第二个问题，若样本 $x$ 在划分属性 $a$ 上的取值已知，则将 $x$ 划分到对应的子节点，且样本权重在子节点中保持为 $w_x$ 。若样本 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划分到所有子节点中，且样本权值在子节点中调整为 $\widetilde{r}_v w_x$ 。直观来说，就是让节点以不同概率划分到子节点中去。
***
#K近邻（KNN）
&emsp;&emsp;K近邻学习是一种常用的监督学习方法，它具有非常简单的原理。给定测试样本，K近邻基于某种距离度量找到训练集中与测试样本最接近的 $k$ 个样本，并基于这 $k$ 个样本的信息对测试样本进行预测。在分类任务中，通常使用投票法，得到 $k$ 个样本中数量最多的类别作为测试样本的类别；在回归任务中，通常将 $k$ 个样本的实值取平均作为测试样本的输出。除此之外还可以使用加权投票法和加权平均，与测试样本越接近的权重越大。
&emsp;&emsp;根据上述的描述我们可以发现，K近邻算法没有显式的训练过程，它仅仅将训练样本保存下来，在测试阶段进行处理。这样的学习方式，训练开销为零。
***
#支持向量机（SVM）
&emsp;&emsp;传说天使和魔鬼玩了一个游戏，魔鬼在桌子上放了两种颜色的球，如下图所示。魔鬼让天使用一根木棍将它们分开，天使不加思索的一摆，将球分成了两部分。随后魔鬼又加入了更多的球，有的球不能再被木棍正确的分开了。
![图一](picture\5aff2bcdbe23a8c764a32b1b5fb13b71_b.png)
&emsp;&emsp;天使重新调整了木棍的摆放位置，使得两边的球都离分割他们的木棍足够远。并且在添加更多的球时，依然可以使加入的球摆放正确。天使调整后的木棒位置，魔鬼按照刚才的方式加入新球，木棍依旧可以很好的将两类球正确的分开。
&emsp;&emsp;接下来，魔鬼给了天使一个新的挑战，按照这种摆法，似乎没有一根木棍可以将他们完美的分开。但天使有法力，他一拍桌子，让这些球都飞到了空中，然后抓起一张纸片，插入了两类球中间，这些球被完美的分开了。
![图二](picture\558161d10d1f0ffd2d7f9a46767de587_b.png)![图三](picture\55d7ad2a6e23579b17aec0c3c9135eb3_b.png)
&emsp;&emsp;上面的例子形象地描述了SVM的分类过程，球就是**数据**，木棍和纸片就是**分类超平面**，找到最优的摆放位置就叫做**优化**，拍飞球叫做**核映射**，将数据从输入空间映射到高维特征空间。
####1.支持向量机的优化目标
&emsp;&emsp;如上面例子中所描述的，SVM的基本思想就是找到一个划分超平面使得不同类别的样本被正确分开。但是能将训练样本划分开的超平面可能有很多，SVM尽可能的找到对未出现样本的泛化能力最强的超平面。
&emsp;&emsp;在样本空间中，超平面可以被表示为 $\bm{w}^T\bm{x}+b=0$,那么样本空间中任意点 $\bm{x}$ 到超平面 $(\bm{w},b)$ 的距离可以表示为 $$ r = \frac{|\bm{w}^T\bm{x} + b|}{||\bm{w}||}. $$
&emsp;&emsp;假设超平面 $(\bm{w},b)$ 可以将训练样本正确分类，即： $$ \begin{cases} \bm{w}^T\bm{x}_i + b \geq +1, y_i = +1; \\ \bm{w}^T\bm{x}_i + b \leq -1,y_i = -1. \end{cases} \tag{1}$$
&emsp;&emsp;定义支持向量为距离超平面最近数据，那么支持向量$(\bm{x}_i,y_i)$必定满足 $ \bm{w}^T\bm{x}_i+b = \pm1 $。两个异类支持向量到超平面的距离之和为 $$ \gamma = \frac{2}{||\bm{w}||}. \tag{2}$$ $\gamma$ 被定义为间隔，如果希望找到具有最大间隔的超平面，也就是找到最大的 $\gamma$ ，即如下所示： $$ \max \limits_{\bm{w},b} \frac{2}{||\bm{w}||} $$ $$ \text{s.t.} \quad y_i(\bm{w}^T\bm{x}_i + b) \geq 1,i=1,2,...,m. \tag{3}$$ 最大化式子(3)可以转化成如下的最小化问题 $$ \min \limits_{\bm{w},b} \frac{1}{2} ||\bm{w}||^2 $$ $$ \text{s.t.} y_i(\bm{w}^T\bm{x}_i + b) \geq 1,i=1,2,...,m. \tag{4} $$ 这个式子就是基本的支持向量机的待优化目标。
####2.优化目标的对偶问题
&emsp;&emsp;SVM的优化目标为通过求解公式(4)得到最优超平面的参数 $\bm{w}$ 和 $b$。为了更高效的求解公式(4)，我们使用拉格朗日乘子法得到其拉格朗日函数： $$ L(\bm{w},b,\bm{\alpha}) = \frac{1}{2} ||\bm{w}||^2 + \sum_{i=1}^{m}{\alpha_i(1 - y_i(\bm{w}^Tx_i + b))}, \tag{5}$$ 其中 $\bm{\alpha} = \{ \alpha_1; \alpha_2;...;\alpha_m \}$， $\alpha_i$ 为拉格朗日乘子。分别对 $L(\bm{w},b,\bm{\alpha})$ 求 $\bm{w}$ 和 $b$ 的偏导为零可得 $$ \bm{w} = \sum_{i=1}^{m}{\alpha_iy_ix_i} $$ $$ 0 = \sum_{i=1}^{m}{\alpha_iy_i} \tag{6}$$ 将式子(6)代入式子(5)中，可以将 $\bm{w}$ 和 $b$ 消去，得到带约束的对偶问题 $$ L(\bm{\alpha}) = \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2} \sum_{i=1}^{m}{\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\bm{x}_i^T\bm{x}_j}} $$ $$ \text{s.t.} \sum_{i=1}^{m}{\alpha_iy_i} = 0, \tag{7} $$ $$ \alpha_i \geq 0, i=1,2,...,m $$ 由凸优化可以知道，对偶问题是原最小化问题的下界，在求解原问题的过程中，我们肯定希望得到原问题的最优下界。最优下界其实就是对偶问题的最大值，即最大化 $L(\bm{\alpha})$。求解对偶问题得到 $\bm{\alpha}$ 后，求解 $\bm{w}$和$b$ 即可得到最优划分超平面。
####3.由优化问题的KKT条件导出支持向量
&emsp;&emsp;在第一节中，我们求解的优化问题为支持向量距离超平面的距离，那么为什么要定义支持向量？为什么只考虑优化支持向量与超平面的距离？
&emsp;&emsp;重新回到公式(4)，该优化问题的约束为不等式问题，因此我们可以得到该优化问题的KKT条件 $$ \begin{cases} \alpha_i \geq 0; \\ y_if(\bm{x}_i) - 1 \geq 0; \\ \alpha_i (y_if(\bm{x}_i) - 1) = 0. \end{cases} $$ 由公式很容易可以看出，对于任意 $(\bm{x}_i,y_i)$ 总有 $\alpha_i = 0$ 或者 $y_if(\bm{x}_i) = 1$。若 $\alpha_i = 0$ ，则该样本不会在优化时进行任何贡献，即样本与划分超平面无关；若 $y_if(\bm{x}_i) = 1$， 则 $\alpha_i \neq 0$，即该样本对优化过程产生了贡献，影响了 $\bm{w}$ 和 $b$ 的值。可以看出，该样本必定在划分超平面的最大间隔边界上，即为我们在之前定义的支持向量。
&emsp;&emsp;从上面的推论可以看出，在SVM的求解过程中，仅仅有一部分的样本对求结果过程产生了影响。而这一部分样本都在划分超平面的最大间隔边界上，即距离划分超平面最近的样本。所以我们将这一部分样本成为支持向量，最终构建得到模型仅仅与支持向量有关。
####4.对偶问题的求解
&emsp;&emsp;很显然，式子(7)的对偶问题最大化可以通过二次规划算法进行求解，但是因为求解的问题大小取决于数据样本数量，在实际中会有很大的时间开销。SMO算法可以将对偶问题高效的求解。
&emsp;&emsp;SMO的思路是是固定 $\alpha_i$ 之外的所有参数，然后求解出 $\alpha_i$ 的极值。但是由于约束 $\sum_{i=1}^{m}{\alpha_iy_i} = 0$ 可以直接导出 $\alpha_i$，所以每次选择两个变量 $\alpha_i$ 和 $\alpha_j$，并固定其他参数。接下来，SMO算法不断的迭代优化参数 $\alpha_i$ 和 $\alpha_j$ 直至他们收敛，迭代过程如下：
1. 选取一对参数 $\alpha_i$ 和 $\alpha_j$；
2. 固定 $\alpha_i$ 和 $\alpha_j$ 以外的参数，求解式子(7)并更新 $\alpha_i$ 和 $\alpha_j$。

&emsp;&emsp;在参数选择的过程中，我们首先选择一个违背KKT条件程度最大的变量，因为违背的程度越大，参数更新后目标函数的增幅就越大。第二个参数我们选择距第一个样本最远的样本，一种解释是两个具有大间距的样本会带来较大的目标函数的更新。
&emsp;&emsp;使用SMO算法求解出参数 $\alpha_i$ 后，即可直接求解得到 $\bm{w}$。注意到对于任意的支持向量 $(\bm{x}_i, y_i)$ 都有 $y_if(\bm{x}_i) = 1$，即 $$ b = \frac{1}{y_i} - \sum_{j \in S}{\alpha_jy_j\bm{x}_j^T\bm{x}_i}. \tag{8}$$ 其中 $S=\{ j | \alpha_j \geq 0,j=1,2,...,m \}$为所有支持向量集合。为了使 $b$ 的值更鲁棒，使用所有支持向量的平均值求解 $b$ $$ b = \frac{1}{|S|} \sum_{i \in S}{(\frac{1}{y_i} - \sum_{j \in S}{\alpha_jy_j\bm{x}_j^T\bm{x}_i})}. \tag{9} $$
####5.核函数与核SVM
&emsp;&emsp;SVM假设所有的训练数据是线性可分的，即模型可以找到一个超平面来将异类样本正确分类。然而很多情况下原始的样本空间并不存在可以将样本正确划分的超平面。如同刚开始我们叙述的天使分类球的问题，当我们讲样本投影到更高维度的特征空间时，样本有较大的概率是线性可分的。所以我们将在原始样本空间线性不可分的数据投影到高维空间，在高维空间中找到划分超平面来将样本正确分类。
&emsp;&emsp;令$\phi(\bm{x})$ 表示将 $\bm{x}$ 映射后的特征向量，于是，在特征空间中划分超平面所对应的SVM模型可以表示为 $$ f(\bm{x}) = \bm{w}^T\phi(\bm{x}) + b, \tag{10} $$ 其中 $\bm{w}$ 和 $b$ 是模型的参数，同式子(4)我们可以得到如下优化公式： $$ \min \limits_{\bm{w},b} \frac{1}{2} ||\bm{w}||^2 \tag{11}$$ $$ \text{s.t.} \quad y_i(\bm{w}^T\phi(\bm{x}_i)+b) \geq 1,i=1,2,...,m. $$ 其对偶问题为 $$ \max \limits_{\bm{\alpha}} \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}{\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\phi(\bm{x}_i)^T\phi(\bm{x}_j)}} \tag{12}$$ $$ \begin{aligned} \text{s.t.} \quad & \sum_{i=1}^{m}{\alpha_iy_i} = 0, \\ & \alpha_i \geq 0,i = 1,2,...,m. \end{aligned}$$
其中 $\phi(\bm{x}_i)^T\phi(\bm{x}_j)$ 为样本 $\bm{x}_i$ 和 $\bm{x}_j$ 映射到特征空间之后的内积。但是当特征空间的维度很高时，计算 $\phi(\bm{x}_i)^T\phi(\bm{x}_j)$ 通常是很复杂的，所以我们进行一个假设，假设存在一个函数 $\kappa$: $$ \kappa (\bm{x}_i,\bm{x}_j) = <\phi(\bm{x}_i), \phi(\bm{x}_j)> = \phi(\bm{x}_i)^T\phi(\bm{x}_j). \tag{13}$$ 即 $\bm{x}_i$ 和 $\bm{x}_j$ 在特征空间的内积等于他们在原始样本空间通过函数 $\kappa$ 计算的结果。
&emsp;&emsp;由上面的假设，我们可以将式子(12)写成包含函数 $\kappa$ 的形式 $$ \max \limits_{\bm{\alpha}} \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2}\sum_{i=1}^{m}{\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_j\kappa (\bm{x}_i,\bm{x}_j)}} \tag{14}$$ $$ \begin{aligned} \text{s.t.} \quad & \sum_{i=1}^{m}{\alpha_iy_i} = 0, \\ & \alpha_i \geq 0,i = 1,2,...,m. \end{aligned}$$ 求解后得到 $$ \begin{aligned} f(\bm{x}) &= \bm{w}^T\phi(\bm{x} + b) \\ &= \sum_{i=1}^{m}{\alpha_iy_i\phi(\bm{x}_i)^T\phi(\bm{x}) + b} \\ &= \sum_{i=1}^{m}{\alpha_iy_i \kappa(\bm{x},\bm{x}_i) + b} \end{aligned} \tag{15}$$ 这里的函数 $\kappa$ 就是核函数。
&emsp;&emsp;因为在实际应用中，我们希望样本可以在特征空间里面线性可分，因此特征空间的好坏对采用核函数的SVM的性能至关重要。于是采用一个合适的核函数在算法的应用中是非常必要的。常见的核函数有如下形式：
|名称|表达式|参数|
|:-:|:-:|:-:|
|线性核|$\kappa(\bm{x}_i,\bm{x}_j) = \bm{x}_i^T\bm{x}_j$|
|多项式核|$\kappa(\bm{x}_i,\bm{x}_j) = (\bm{x}_i^T\bm{x}_j)^d$|$d \geq 1$为多项式的次数|
|高斯核|$\kappa(\bm{x}_i,\bm{x}_j) = \exp(-\frac{\|\bm{x}_i-\bm{x}_j\|^2}{2 \sigma ^2})$|$\sigma>0$为高斯核的核宽|
|拉普拉斯核|$\kappa(\bm{x}_i,\bm{x}_j) = \exp(-\frac{\|\bm{x}_i-\bm{x}_j\|}{\sigma})$|$\sigma>0$|
|$Sigmoid$核|$\kappa(\bm{x}_i,\bm{x}_j) = tanh(\beta \bm{x}_i^T\bm{x}_j+\theta)$|$tanh$为双曲正切函数，$\beta>0,\theta>0$|
&emsp;&emsp;此外核函数还可以通过函数组合得到：
1. 若$\kappa_1$和$\kappa_2$为核函数，则对于任意正数$\gamma_1$和$\gamma_2$，其线性组合$\gamma_1\kappa_1+\gamma_2\kappa_2$也是核函数；
2. 若$\kappa_1$和$\kappa_2$为核函数，则核函数的直积$\kappa_1 \bigotimes \kappa_2(\bm{x},\bm{z}) = \kappa_1(\bm{x},\bm{z})\kappa_2(\bm{x},\bm{z})$也是核函数；
3. 若$\kappa_1$为核函数，则对任意函数$g(\bm{x})$,$ \kappa(\bm{x},\bm{z}) = g(\bm{x})\kappa_1(\bm{x},\bm{z})g(\bm{z})$也是核函数。
####5.软间隔与正则化
&emsp;&emsp;在前面的介绍中，我们假设样本在样本空间或者特征空间中必定是线性可分的，即存在一个超平面可以将样本完全正确的划分开来。然而在实际数据中总存在这噪声或者离群点，很难在样本空间或者特征空间中找到合适的超平面来使的数据被完全正确分类，并且在优化的过程中，很容易出现数据的过拟合问题。为了缓解这些问题，我们在SVM中允许一部分的样本出错，即引入软间隔。
&emsp;&emsp;在前面的介绍中，我们要求超平面必须将所有的样本分类正确，这叫做硬间隔；软间隔就是允许一部分样本不满足约束 $$ y_i(\bm{w}^T\bm{x}_i + b) \geq 1. \tag{16}$$ 于是SVM的优化目标可以写成 $$ \min \limits_{\bm{w},b} \frac{1}{2} \|\bm{w}\|^2 + C\sum_{i=1}^{m}{\iota_{0/1}(y_i(\bm{w}^T\bm{x}_i + b)-1)}, \tag{17}$$ 其中 $C>0$ 是一个常数，$\iota_{0/1}$ 是 $0/1$ 损失函数 $$ \iota_{0/1} = \begin{cases} 1, \quad \text{if $z<0$}; \\ 0, \quad \text{otherwise}. \end{cases} $$ 显然在这个优化问题中，$C$ 越大，允许违背约束的样本数量越少，当 $C$ 为无穷大时，迫使所有样本都满足约束，即退化到硬间隔。
&emsp;&emsp;但是 $\iota_{0/1}$ 非凸、非连续，数学性质不好，不易直接求解，所以通常使用其他函数进行替代。替代函数通常是凸的连续函数，并且是 $\iota_{0/1}$ 的上界：
1. $hinge$ 损失：$\iota_{hinge}(z) = max(0,1-z);$
2. 指数损失：$\iota_{exp}(z) = \exp(-z);$
3. 对率损失：$\iota_{log}(z) = \log(1+\exp(-z)).$

&emsp;&emsp;引入松弛变量 $\xi \geq 0$ 代表损失函数，式子(17)被重写为 $$ \min \limits_{\bm{w},b,\xi_i} \frac{1}{2} \|\bm{w}\|^2 + C \sum_{i=1}^{m}{\xi_i} \tag{18}$$ $$ \begin{aligned} \text{s.t.} \quad & y_i(\bm{w}^T\bm{x}_i + b) \geq 1 - \xi_i \\ & \xi_i \geq 0, \, i = 1,2,...,m \end{aligned} $$ 这就是软间隔支持向量机。其中 $\xi_i$ 为第 $i$ 个样本不满足约束(16)的程度。通过拉格朗日乘子法，我们可以得到 $$ \begin{aligned} L(\bm{w},b,\bm{\alpha},\bm{\xi},\bm{\mu})
&= \frac{1}{2} \|\bm{w}\|^2 + C\sum_{i=1}^{m}{\xi_i} \\ &= \sum_{i=1}^{m}{\alpha_i(1-\xi_i-y_i(\bm{w}^T\bm{x}_i+b)) - \sum_{i=1}^{m}{\mu_i\xi_i}}, \end{aligned} \tag{19} $$ 其中 $ \alpha_i \geq 0, \mu_i \geq 0 $。令 $L(\bm{w},b,\bm{\alpha},\bm{\xi},\bm{\mu})$ 对 $\bm{w}, b, \xi_i$ 的偏导为零可得 $$ \bm{w} = \sum_{i=1}^{m}{\alpha_iy_i\bm{x}_i}, $$ $$ 0 = \sum_{i=1}^{m}{\alpha_iy_i}, $$ $$ C = \alpha_i + \mu_i. $$ 同理可以得到式子(18)的对偶问题 $$ \max \limits_{\bm{\alpha}} \sum_{i=1}^{m}{\alpha_i} - \frac{1}{2} \sum_{i=1}^{m}{\sum_{j=1}^{m}{\alpha_i\alpha_jy_iy_i\bm{x}_i^T\bm{x}_j}} \tag{20}$$ $$ \begin{aligned} \text{s.t.} \quad & \sum_{i=1}^{m}{\alpha_iy_i} = 0, \\ & 0 \leq \alpha_i \leq C,\; i=1,2,...m. \end{aligned} $$ 将式子(20)和硬间隔下的对偶问题式子(7)对比可以看出，两者的唯一区别就是对变量 $\alpha_i$ 的约束不同，于是可以使用求解式子(7)的方法求解式子(20)，同理可以得到引入核函数后的软间隔对偶问题。
&emsp;&emsp;类似于硬间隔方法，我们可以得到软间隔方法的KKT条件 $$ \begin{cases} \alpha_i \geq 0,\quad \mu_i \geq 0, \\ y_if(\bm{x}_i) - 1 + \xi_i \geq 0, \\ \alpha_i(y_if(\bm{x}_i) - 1 + \xi_i) = 0, \xi_i \geq 0, \quad \mu_i\xi_i = 0. \end{cases} \tag{21}$$ 可以看出，对于任意训练样本$(\bm{x}_i,y_i)$，总有 $\alpha_i = 0$ 或者 $y_if(\bm{x}_i) = 1 - \xi_i$。若 $\alpha_i = 0$，则该样本不会对$f(\bm{x})$有任何影响；若 $\alpha_i > 0$，则必然有 $y_if(\bm{x}_i) = 1 - \xi_i$，则该样本是支持向量。
&emsp;&emsp;由 $C = \alpha_i + \mu_i$ 可知，若 $\alpha_i<C$，则 $\mu_i>0$，进而有 $\xi_i=0$，则该样本恰在最大间隔边界上；若 $\alpha_i=C$，则有 $\mu_i=0$，此时若 $\xi_i \leq 1$ 则样本落在最大间隔内部，若 $\xi_i > 1$ 则该样本被错误分类。
*** 
#聚类算法的评估
&emsp;&emsp;相对于监督学习，无监督学习通常没有标注数据，算法的设计直接影响最终的输出和模型的性能。首先我们先介绍常见数据簇的特点。
1. **以中心定义的数据簇**：这类数据集合倾向于球状分布，即数据簇中的所有数据的平均值为簇的中心。簇中的数据到中心的距离比到其它簇中心的距离更近。
2. **以密度定义的数据簇**：这类数据簇呈现出和其他数据簇不同的密度，或稠密或稀疏。当数据簇不规则或者相互盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义。
3. **以连通定义的数据簇**：这类数据的数据点和数据点之间有联通关系，整个数据簇表现为图结构。通常为不规则形状或者缠绕的数据簇。
4. **以概念定义的数据簇**：这类数据集合中的所有数据点具有某种共同性质。
由于数据以及需求的多样性，没有一种算法可以适用于所有的数据类型、数据簇或应用场景。每种情况可能都需要一种不同的评估方法，在很多情况下，判断聚类算法结果的好坏强烈依赖主观解释。但聚类算法的评估还是非常必要的。

&emsp;&emsp;聚类的评估任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结果的质量。这一过程通常分为三部分：
1. **估计聚类趋势**。这一步骤检测数据分布中是否存在随机的簇结构。如果数据的随机分布的，那对数据行行聚类是毫无意义的。我们可以观察聚类误差是否随着聚类类数的增加而单调变化。如果数据分布是随机的，那么聚类误差的变化幅度应该不大。还可以使用**霍普金斯统计量**来判断数据的随机性。
2. **判定数据簇数**。确定聚类趋势之后，我们需要找到与真实数据分布最为吻合的簇数，由此来判定聚类结果的质量，如**手肘法**和**GapStatistic**。用于评估的最佳数据簇数可能与算法输出的簇数是不同的。
3. **确定聚类效果**。在无监督的情况下，我们可以通过考察簇之间的分离情况和簇内部的紧凑情况来评估聚类的效果。为了更合理的评估不同聚类算法的性能，通常还需要认为地构造不同类型的数据集，以观察聚类算法在这些数据集上的效果。
***
&emsp;&emsp;聚类算法通常分为原型聚类、密度聚类和层次聚类。K均值聚类、高斯混合聚类都为原型聚类。
***
#K均值聚类
####1.K均值聚类的具体步骤
&emsp;&emsp;原型聚类算法通常先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式，将产生不同的算法。K均值聚类是一种原型聚类算法，它的基本思想是通过迭代的方式求解K个簇划分，使得聚类结果对应的代价函数最小。算法的具体步骤如下：
1. 数据预处理，如归一化、离群点处理等。
2. 随机选取K个簇中心，记为 $ \mu_1^{(0)} ,\mu_2^{(0)}, ..., \mu_K^{(0)} $。
3. 定义代价函数： $ J(c,\mu) = \min \limits_{\mu} \min \limits_{c} \sum_{k=1}^{K}{\sum_{x_i \in c_k}{||x_i - \mu_{k}||^2}} $。
4. 令 $t=0,1,2,....$ 为迭代步数，重复下面过程直到 $J$ 收敛：
   对于每一个样本 $x_i$，将其分配到距离最近的簇：$$ c_i^{(t)} = \argmin \limits_k ||x_i - \mu_k^{(t)}||^2; $$
   对于每一个类簇 $k$，重新计算该类簇的中心：$$ \mu_k^{(t+1)} = \frac{1}{|c_k|}  \sum_{x_i \in c_k^{(t)}}{x_i}; $$
   K均值算法的 ${c_i}$ 和 ${\mu_k}$ 交替更新，直到 ${\mu_k}$ 不再更新为止，即 ${\mu_k}$收敛。当 ${\mu_k}$ 收敛时，${c_i}$ 收敛，同时 ${J}$ 也递减到最小值。
####2.k均值聚类的优缺点
&emsp;&emsp;由于K均值聚类初始值的影响，会出现聚类结果不稳定、结果通常是局部最优而不是全局最优的情况。并且无法很好的解决数据簇分布差别比较大的情况，不太适用于离散分类。但由于k均值聚类的时间复杂度 $O(NKt)$ 接近线性,所以对于大数据集，k均值聚类很高效。
&emsp;&emsp;K均值聚类本质上是一种基于欧氏距离的数据划分方法，均值和方差的维度都会对最后的聚类结果产生很大的影响，所以未做归一化处理的数据是无法参与运算和比较的。同时离群点和噪声数据会对均值造成较大的影响，导致数据簇中心的偏移，因此通常在做k均值聚类之前通常需要对数据做**预处理**。
&emsp;&emsp;由于K均值聚类的初始值会对算法的效果有较大的影响，所以**K值的选择**是K均值聚类最大的问题之一。通产来说，K值的选择一般基于经验和多次实验结果（手肘法、GapStatistic）。
&emsp;&emsp;K均值算法在本质上假设各个数据簇呈现球型或者高维球形分布。当数据簇呈现非凸的分布形状时，可以引入**核函数**来进行优化。核函数可以将数据点映射到高维特征空间，在高维特征空间中，数据线性可分的概率大大增加。
####3.改进k均值聚类
1. **K-means++算法**
   原始K均值算法最开始随机选取数据集中 $K$ 个点作为聚类中心，而K-means++算法按照如下思想选取聚类中心。首先在选取第一个聚类中心时通过随机的方法，随后的聚类中心较大概率的选择与之前的聚类中心较远的点。随后的操作和原始K均值算法相同。
2. **ISODATA算法**
   ISODATA的全称是迭代自组织数据分析法。ISODATA算法针对K均值聚类算法的 $K$ 值选择进行优化。当属于某个类别的样本数过少时，算法将该类去除；当属于某个类的样本数过多、分散程度较大时，把该类别分成两个子类别。但ISODATA算法需要指定过多参数：参考聚类数量 $k_0$、每个类所要求的最少样本数量 $N_{min}$、最大方差 $\sigma$以及两个聚类中心所允许的最小距离 $D_{min}$。
***
#高斯混合聚类（高斯混合模型）
####1.高斯混合模型
&emsp;&emsp;高斯混合模型的核心思想是，假设数据可以看作是从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准的高斯模型，模型的均值 $\mu_i$ 和方差 $\Sigma_i$ 都是待估计的参数。每一个分模型还有一个参数 $\alpha_i$ 代表该分模型的权重或者是生成数据的概率。则高斯混合模型的分布为 $$ p(x) = \sum_{i=1}^{k}{\alpha_i p(x|\mu_i,\Sigma_i)} \tag{1}.$$ 其中 $\alpha_i > 0$ 并且 $\sum_{i=1}^{k}{\alpha_i} = 1$。该分布共有 $k$ 个混合成分组成，每个混合成分对应一个高斯分布。
&emsp;&emsp;若训练集符合高斯混合模型的假设，令随机变量 $\theta_j \in \{ 1,2,...,k \}$ 表示生成样本 $x_j$ 的高斯混合成分。显然 分模型生成数据的概率 $\alpha_i$ 即为 $\theta_j$ 的先验概率 $p(\theta_j = i)$。根据贝叶斯定理，$\alpha_i$ 的后验分布为： $$ \begin{aligned} p(\theta_j=i|x_j) &= \frac{p(\theta_j = i) p(x_j|\theta_j = i)}{p(x_j)} &= \frac{\alpha_i p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^{k}{p(x_j|\mu_l,\Sigma_l)}} \end{aligned} \tag{2}$$ $p(\theta_j=i|x_j)$ 定义为样本 $x_j$ 由第 $i$ 个高斯混合成分生成的后验概率。我们将其定义为 $\gamma_{ji}$。
&emsp;&emsp;当高斯混合成分已知时，高斯混合聚类将把样本集 $D$ 划分成 $k$ 个簇，每个样本 $x_j$ 的簇标记 $\lambda_j$ 确定如下： $$ \lambda_j = \argmax \limits_{i \in \{ 1,2,...,k \}} \gamma_{ji}. \tag{3}$$ 由此可以看出，高斯混合模型是采用概率模型对数据簇原型进行刻画，为簇划分则由原型对应的后验概率确定。
&emsp;&emsp;对于模型参数，我们使用最大似然估计进行求解。$$ L(D) = ln(\prod_{j=1}^{n}{p(x_j)}) = \sum_{j=1}^{n}{\ln\left( \sum_{i=1}^{k}{\alpha_ip(x_j|\mu_i,\Sigma_i)} \right)} \tag{4}$$ 对上述公式进行求偏导,并限制 $\sum_{i=1}^{k}{\alpha_i} = 1$，可以得到 $$ \mu_i = \frac{\sum_{j=1}^{n}{\gamma_{ji}x_j}}{\sum_{j=1}^{n}{\gamma_{ji}}}, $$ $$ \Sigma_i = \frac{\sum_{j=1}^{n}{\gamma_{ji} (x_j - \mu_i)(x_j - \mu_i)^T}}{\sum_{j=1}^{n}{\gamma_{ji}}}, $$ $$ \alpha_i = \frac{1}{m} \sum_{j=1}^{n}{\gamma_{ji}}. \tag{5}$$
&emsp;&emsp;接下来使用EM算法进行迭代优化求解三个参数：在每步迭代中，现根据当前参数计算每个样本属于每个高斯成分的后验概率 $\gamma_{ji}$，再根据公式(5)更新模型参数。一直重复该迭代过程直到满足停止的条件（达到最大迭代轮数、似然函数 $L(D)$ 增长很少或者不在增长）为止。当高斯混合模型的所有分模型参数确定后，就可以根据公式(3)来进行簇划分。
####2.高斯混合模型和k均值算法
&emsp;&emsp;高斯混合模型和k均值算法的相同点是，它们都是聚类算法；都需要指定 $K$ 值；都使用EM算法来求解；通常只能收敛到局部最优。
&emsp;&emsp;高斯混合模型相对于K均值算法的优点是，可以给出一个样本属于某个数据簇的概率；不仅可以用于聚类，还可以用于概率密度估计；可以用于生成新的样本点。
***
#DBSCAN
***
#AGNES
***
#主成份分析（PCA）
&emsp;&emsp;在机器学习领域，对原始数据进行特征提取，可能会得到比较高维度的特征向量。在这些向量所在的高维空间，包含很多冗余和噪声。我们希望通过降维的方法来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。主成份分析作为降维中最经典的方法，是一种线性、非监督、全局的降维方法。
####1.最近重构性——最小平方误差
&emsp;&emsp;给定一系列样本点，怎么样才能用一个超平面来对所有样本进行恰当的表示呢？线性回归目标是求解出一个线性函数，使线性函数对应的直线可以更好的拟合样本。那么模仿线性回归，并扩展到高维空间，使用超平面对所有样本进行恰当的表示其实是找到一个 $d$ 维超平面，使得所有数据点到这个超平面的距离和最小。令 $d=1$ 该问题就退化成了求解一条直线，使所有点到该直线的距离平方和最小。
&emsp;&emsp;假设数据集中每个点 $x_k$ 到 $d$ 维超平面的距离为 $$ distance(x_k,D) = {|| x_k - \widetilde{x_k} ||}_2 \tag{1} $$ 其中 $ \widetilde{x_k} $ 表示 $x_k$ 在超平面 $D$ 上的投影向量。根据线性代数，假设超平面是由一组正交基 $ W=\{w_1,w_2,...,w_d\} $ 构成，那么 $ \widetilde{x_k} $ 可以由这组正交基线性表示 $$ \widetilde{x_k} = \sum_{i=1}^{d}{({w_i}^Tx_k)w_i} \tag{2}$$ 其中 $w_i^Tx_k$ 表示 $x_k$ 在 $w_i$ 方向上投影的长度。实际上， $ \widetilde{x_k} $ 就是 $x_k$ 在 $ W $ 这组正交基表示的超平面上的坐标。
&emsp;&emsp;根据最近重构性，可以得到PCA待优化的目标为找到一组 $W$， 使得所有数据点到 $W$ 所代表的超平面的距离和最小。即 $$ \begin{cases} \argmin_{w_1,w_2,...w_d}{\sum_{k=1}^{n}{{||x_k- \widetilde{x_k}||}_2^2}}, \\ \text{s.t. } {w_i}^Tw_j = \begin {cases} 1,i=j; \\ 0,i\neq{j}. \end {cases} \end{cases} \tag{3}$$ 
&emsp;&emsp;将式(3)中的距离展开 $$ \begin{aligned} ||x_k - \widetilde{x_k}||_2^2 &= (x_k - \widetilde{x_k})^T(x_k - \widetilde{x_k}) \\ &= {x_k}^Tx_k - 2{x_k}^T\widetilde{x_k} + \widetilde{x_k}^T\widetilde{x_k} \end{aligned} \tag{4} $$ 其中第一项与 $W$ 无关，所以当作常数看待。将式(2)带入式(4)中的后两项可以得到 $$ \begin{aligned} {x_k}^T\widetilde{x_k} &= {x_k}^T\sum_{i=1}^{d}{({w_i}^Tx_k)w_i} \\ &= \sum_{i=1}^{d}{{w_i}^Tx_k{x_k}^Tw_i } \end{aligned} $$  $$ \begin{aligned} \widetilde{x_k}^T\widetilde{x_k} &= \left( \sum_{i=1}^{d}{({w_i}^Tx_k)w_i} \right)^T \left( \sum_{i=1}^{d}{({w_i}^Tx_k)w_i} \right) \\ &= \sum_{i=1}^{d}{(({w_i}^Tx_k)w_i)^T(({w_i}^Tx_k)w_i)} \\ &= \sum_{i=1}^{d}{({w_i}^Tx_k)({w_i}^Tx_k)} = \sum_{i=1}^{d}{({w_i}^Tx_k)({x_k}^Tw_i)} \\ &=\sum_{i=1}^{d}{{w_i}^Tx_k{x_k}^Tw_i} \end{aligned} $$
&emsp;&emsp;注意到 $\sum_{i=1}^{d}{{w_i}^Tx_k{x_k}^Tw_i}$ 实际上就是矩阵 $W^Tx_k{x_k}^TW$ 的迹，于是可以将式(4)继续化简 $$ ||x_k - \widetilde{x_k}||_2^2 = -\sum_{i=1}^{d}{{w_i}^Tx_k{x_k}^Tw_i} + {x_k}^Tx_k $$ 所有数据的距离和就等于 $$ \begin{aligned} \sum_{k=1}^{n}{||x_k - \widetilde{x_k}||_2^2} &= -\sum_{i=1}^{d}{{w_i}^T \sum_{k=1}^{n}{(x_k{x_k}^T)}w_i} + \sum_{k=1}^{n}{{x_k}^Tx_k} \\ &= -\sum_{i=1}^{d}{{w_i}^T XX^Tw_i} + C \end{aligned}  $$
&emsp;&emsp;进一步我们可以把优化问题(3)转换成如下形式 $$ \begin{cases} \argmin_{w_1,w_2,...w_d}{-\sum_{i=1}^{d}{{w_i}^TXX^Tw_i}}, \\ \text{s.t. } {w_i}^Tw_j = \begin {cases} 1,i=j; \\ 0,i\neq{j}. \end {cases} \end{cases} \tag{5}$$
####2.最大可分性——最大方差
&emsp;&emsp;PCA旨在找到数据中的主成份，并利用这些主成份表征数据。使用数据的主成份来表征数据可以使信息损失达到最少。如果将数据的主成份看作是一个超平面，那么数据在该超平面上的投影将尽可能的分开。数据的投影尽可能的分开意味着数据在该超平面上尽可能的分散，也就意味着数据在这个超平面上每个正交基上投影的方差都尽可能的大。
&emsp;&emsp;假定所有数据进行了中心化，即均值为0。 数据 $x_k$ 在超平面上的投影为 $W^Tx_k$ 其中， $ W=\{w_1,w_2,...,w_d\} $ 为超平面的正交基向量构成的矩阵。因此所有数据在正交基 $w_i$上投影的方差为： $$ \begin{aligned} D(x,w_i) = \frac{1}{n} \sum_{k=1}^{n}{({w_i}^Tx_k)^2} &= \frac{1}{n} \sum_{k=1}^{n}{({w_i}^Tx_k)({w_i}^Tx_k)^T} \\ &= \frac{1}{n} \sum_{k=1}^{n}{{w_i}^Tx_kx_k^Tw_i} \\ &= {w_i}^T \left( \frac{1}{n} \sum_{k=1}^{n}{x_k{x_k}^T} \right) w \\ &= \frac{1}{n} {w_i}^TXX^Tw \end{aligned} \tag{6} $$ 显然 $XX^T$ 是样本的协方差矩阵，因此我们可以根据最大可分性得到如下优化条件： $$ \begin{cases} \argmax_{W}{{w_i}^T XX^T w}, \\ \text{s.t. } WW^T = I. \end{cases} \tag{7} $$
####3.优化方程的求解
&emsp;&emsp;通过使用最近重构性和最大可分性分别获得优化问题(5)和(7)。显然式子(5)可以分解分求 $d$ 项 $-{w_i}^TXX^Tw_i$ ，即和式子(7)的优化问题相同。接下来通过描述求解式子(7)来介绍PCA优化问题的求解。
&emsp;&emsp;首先对公式(7)使用拉格朗日乘子法可得 $$ XX^Tw_i = \lambda w_i \tag{8} $$ 公式(8)很容易看出来 $w_i$ 的解为协方差矩阵 $XX^T$ 的特征向量，并且最大的方差就位协方差矩阵最大的特征值。由此我们可以计算出数据的协方差矩阵特征值前 $d$ 大的特征值对应的特征向量作为最优的超平面。
&emsp;&emsp;由此我们可以总结PCA的求解过程如下：
1. 对样本数据进行预处理。
2. 求样本数据的协方差矩阵。
3. 对协方差矩阵进行特征值分解，并将特征值按照从大到小顺序排列。
4. 取特征值前 $d$ 大对应的特征向量 $w_1,w_2,...w_d$ ，并将数据映射到 $d$ 维超平面上。

&emsp;&emsp;作为最常用的降维方法，怎么样确定降维后的特征维度 $d$ 是很关键的。可以通过使用不同的 $d$ 值对开销较小的分类器进行交叉验证，选取效果最好的值。另外还可以通过降维后的信息占比来确定 $d$。定义降维后的信息占比为 $ \eta = \sqrt{\frac{\sum_{i=1}^{d}{{\lambda_i}^2}}{\sum_{i=1}^{n}{{\lambda_i}^2}}} $。可以设置一个阈值，并找到使信息占比大于等于阈值的最小的 $d$ 值。
***
#线性判别分析(LDA)
&emsp;&emsp;线性判别分析是一种有监督学习算法，同时也经常被用来进行数据降维。
####1.二分类问题
&emsp;&emsp;LDA是有监督的算法，核心思想是最大化类间距离和最小化类内距离。我们从简单的二分类问题出发，并在下一节中扩展到多分类问题。
&emsp;&emsp;假设有 $C_1$、$C_2$ 两个类别的样本，两种类别的均值分别为 $\mu_1=\frac{1}{N}\sum_{x\in C_1}{x}$,$\mu_2=\frac{1}{N}\sum_{x\in C_2}{x}$。若希望投影后样本两类之间的距离尽可能大，距离表示为 $$ D(C_1,C_2) = ||\widetilde{\mu_1}-\widetilde{\mu_2}||_2^2 \tag{1}$$ 其中 $\widetilde{\mu_1}$,$\widetilde{\mu_2}$ 表示两个类中心在 $w$ 上的投影向量。 $\widetilde{\mu_1} = w^T \mu_1$,$\widetilde{\mu_2} = w^T \mu_2$, 因此，最大化类间距离可以表示为下面式子 $$ \begin{cases} \max \limits_{w} {||w^T(\mu_1-\mu_2)||_2^2}, \\\text{s.t.} w^Tw=1. \end{cases} \tag{2}$$ 容易发现，当 $w$ 与 $\mu_1-\mu_2$ 方向一致时，该距离达到最大值。
&emsp;&emsp;为了得到数据集的类内距离，我们将全部数据的类内距离定义为各个类分别的方差之和。 $$D(C_1) = \sum_{x\in C_1}{(w^Tx-w^T\mu_1)^2} = \sum_{x\in C_1}{w^T(x-\mu_1)(x-\mu_1)^Tw} $$ $$D(C_2) = \sum_{x\in C_2}{w^T(x-\mu_2)(x-\mu_2)^Tw} \tag{3}$$
&emsp;&emsp;接下来，我们定义LDA的优化目标为最大化类间距离和类内距离的比值。 $$\begin{aligned} \max \limits_{w} J(w) &= \frac{||w^T(\mu_1-\mu_2)||_2^2}{\sum_{i=1}^{2}{\sum_{x\in C_i}{w^T(x-\mu_i)(x-\mu_i)^Tw}}} \\ &= \frac{w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw}{\sum_{i=1}^{2}{\sum_{x\in C_i}{w^T(x-\mu_i)(x-\mu_i)^Tw}}} \end{aligned}  \tag{4}$$ 定义两类数据的类间散度矩阵 $S_b=(x-\mu_i)(x-\mu_i)^T$，类内散度矩阵为两个类别的散度矩阵的和 $S_w=\sum_{i=1}^{2}{\sum_{x\in C_i}{(x-\mu_i)(x-\mu_i)^T}}$。则式子(4)可以重新写成 $$\max \limits_{w} J(w) = \frac{w^TS_bw}{w^TS_ww} \tag{5}$$
&emsp;&emsp;对式子(5)求导，令倒数等于0。可以解得 $$ (w^TS_ww)S_bw = (w^TS_bw)S_ww \tag{6}$$ 由于 $(w^TS_ww)$ 和 $(w^TS_bw)$ 是两个标量,我们令 $\lambda = J(w) = \frac{w^TS_bw}{w^TS_ww} $。可以将式子(6)转化成下列形式： $$ S_bw = \lambda S_ww $$ $$ S_w^{-1}S_bw = \lambda w \tag{7}$$
&emsp;&emsp;可以看出，在对优化方程(5)求解的过程中，转化得到了一个求矩阵特征向量的问题。 $J(w)$ 就对应着矩阵 $S_w^{-1}S_b$ 最大的特征值，最优 $w$ 就是最大特征值对应的特征向量。
&emsp;&emsp;可以发现，类间散度矩阵 $S_b = (\mu_1-\mu_2)(\mu_1-\mu_2)^T$，因此 $S_b$ 和 $(\mu1-\mu_2)$ 的方向始终一致（$(\mu_1-\mu_2)w$为一标量）。如果我们只考虑 $w$ 的方向，可以直接求得 $w=S_s^{-1}(\mu_1-\mu_2)$。
####2.多分类问题
&emsp;&emsp;将LDA扩展到多类情况，假设有 $c$ 个类别，并且需要将特征降到 $d$ 维。我们需要找到一个 $d$ 维的超平面 $W = \{ w_1, w_2,....w_d \}$ 来使投影后的样本满足LDA的目标。
&emsp;&emsp;由于多类问题中，类间散度矩阵无法定义，所以我们引入全局散度矩阵 $$ \begin{aligned} S_t &= S_b + S_w \\ &= \sum_{i=1}^{n}{(x_i-\mu)(x_i-\mu)^T} \end{aligned} $$ 其中， $\mu$ 是全部数据的均值向量。由此可以得到 $$ \begin{aligned} S_b &= S_t - S_w \\ &= \sum_{i=1}^{n}{(x_i-\mu)(x_i-\mu)^T} - \sum_{i=1}^{c}{\sum_{x\in C_i}{(x-\mu_i)(x-\mu_i)^T}} \\ &= \sum_{i=1}^{c}{ \left( \sum_{x\in C_i}{(x-\mu)(x-\mu)^T} - \sum_{x\in C_i}{(x-\mu_i)(x-\mu_i)^T} \right)} \\ &= \sum_{i=1}^{c}{m_i(\mu_i-\mu)(\mu_i-\mu)^T} \end{aligned} $$ 其中 $m_i$ 是第 $i$ 类的样本个数。由上式可以看出，类间散度表示的就是每个类别的中心到全局中心的加权距离。最大化类间散度实际上就是在投影后，使每个类别的中心距离全局中心足够远。
&emsp;&emsp;根据LDA的定义，我们可以将优化目标定义如下： $$ J(w) = \frac{tr(W^TS_bW)}{tr(W^TS_wW)} $$ 得到 $S_b$ 和 $S_w$ 后，最优解 $W = \{ w_1, w_2,....w_d \}$可以通过求解 $S_w^{-1}S_b$ 的前 $d$ 个最大的特征向量对应的特征值来得到。
####3.PCA与LDA的区别和联系。
&emsp;&emsp;作为两个最常用的降维方法，PCA和LDA都假设数据的分布服从正态分布。虽然从求解优化方程的过程看，PCA和LDA都是用了特征值分解的方法，但是两种方法的基本思想存在着很大的不同。由于PCA是无监督方法，它假设方差越大，信息量越多。通过最大化数据方差来找到最优超平面来达到降维的效果。而LDA是有监督的学习方法，它选择数据投影后类内方差最小、类间方差最大的方向。它用到了类别信息，使得不同类别的数据投影后尽可能被分开。
&emsp;&emsp;从实际应用方面来看，PCA主要用于用于去除噪声，进行特征选择；而LDA可以得到分类特征，使得降维后的特征具有更好的可分性。并且LDA可以用于分类问题。